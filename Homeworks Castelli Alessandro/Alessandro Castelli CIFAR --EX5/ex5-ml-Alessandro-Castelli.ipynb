{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3649,"databundleVersionId":46718,"sourceType":"competition"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alessandro Castelli Kaggle: https://www.kaggle.com/code/alessandromajumba/ex5-ml","metadata":{}},{"cell_type":"markdown","source":"# Kaggle Competition: https://www.kaggle.com/competitions/cifar-10","metadata":{}},{"cell_type":"markdown","source":"# WANDB: https://wandb.ai/ales-2000-09/CIFAR-10?workspace=user-ales-2000-09","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-01-22T22:48:51.069190Z","iopub.execute_input":"2024-01-22T22:48:51.069603Z","iopub.status.idle":"2024-01-22T22:48:52.056828Z","shell.execute_reply.started":"2024-01-22T22:48:51.069566Z","shell.execute_reply":"2024-01-22T22:48:52.054871Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"/kaggle/input/cifar-10/trainLabels.csv\n/kaggle/input/cifar-10/sampleSubmission.csv\n/kaggle/input/cifar-10/test.7z\n/kaggle/input/cifar-10/train.7z\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import everything needed","metadata":{}},{"cell_type":"code","source":"import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport collections\nimport math\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import datasets, transforms\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:52.061192Z","iopub.execute_input":"2024-01-22T22:48:52.063351Z","iopub.status.idle":"2024-01-22T22:48:53.006907Z","shell.execute_reply.started":"2024-01-22T22:48:52.063261Z","shell.execute_reply":"2024-01-22T22:48:53.005889Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"markdown","source":"## Unzip datasets","metadata":{}},{"cell_type":"code","source":"#!pip install py7zr","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:53.008308Z","iopub.execute_input":"2024-01-22T22:48:53.008702Z","iopub.status.idle":"2024-01-22T22:48:54.219000Z","shell.execute_reply.started":"2024-01-22T22:48:53.008664Z","shell.execute_reply":"2024-01-22T22:48:54.217882Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"# WARNING: It can take a lot of time to uncompress!","metadata":{}},{"cell_type":"code","source":"#!python -m py7zr x /kaggle/input/cifar-10/train.7z","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:54.221434Z","iopub.execute_input":"2024-01-22T22:48:54.221962Z","iopub.status.idle":"2024-01-22T22:48:55.166033Z","shell.execute_reply.started":"2024-01-22T22:48:54.221921Z","shell.execute_reply":"2024-01-22T22:48:55.165006Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"#!python -m py7zr x /kaggle/input/cifar-10/test.7z","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:55.167395Z","iopub.execute_input":"2024-01-22T22:48:55.167794Z","iopub.status.idle":"2024-01-22T22:48:56.159251Z","shell.execute_reply.started":"2024-01-22T22:48:55.167756Z","shell.execute_reply":"2024-01-22T22:48:56.158247Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"data_dir = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:56.160586Z","iopub.execute_input":"2024-01-22T22:48:56.160943Z","iopub.status.idle":"2024-01-22T22:48:57.067857Z","shell.execute_reply.started":"2024-01-22T22:48:56.160909Z","shell.execute_reply":"2024-01-22T22:48:57.066878Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Label\")\n\nimport wandb\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:57.069211Z","iopub.execute_input":"2024-01-22T22:48:57.069557Z","iopub.status.idle":"2024-01-22T22:48:58.961432Z","shell.execute_reply.started":"2024-01-22T22:48:57.069523Z","shell.execute_reply":"2024-01-22T22:48:58.960523Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def read_csv_labels(fname):\n    \"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\n    with open(fname, 'r') as f:\n        # Skip the file header line (column name)\n        lines = f.readlines()[1:]\n    tokens = [l.rstrip().split(',') for l in lines]\n    return dict(((name, label) for name, label in tokens))\n\nlabels = read_csv_labels(os.path.join(data_dir, '/kaggle/input/cifar-10/trainLabels.csv'))\nprint(f'Number training examples: {len(labels)}')\nprint(f'Number classes: {len(set(labels.values()))}')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:58.962635Z","iopub.execute_input":"2024-01-22T22:48:58.962972Z","iopub.status.idle":"2024-01-22T22:48:59.998330Z","shell.execute_reply.started":"2024-01-22T22:48:58.962937Z","shell.execute_reply":"2024-01-22T22:48:59.997367Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Number training examples: 50000\nNumber classes: 10\n","output_type":"stream"}]},{"cell_type":"code","source":"def copyfile(filename, target_dir):\n    \"\"\"Copy a file into a target directory.\"\"\"\n    os.makedirs(target_dir, exist_ok=True)\n    shutil.copy(filename, target_dir)\n\ndef reorg_train_valid(data_dir, labels, valid_ratio):\n    \"\"\"Split the validation set out of the original training set.\"\"\"\n    # The number of examples of the class that has the fewest examples in the\n    # training dataset\n    n = collections.Counter(labels.values()).most_common()[-1][1]\n    # The number of examples per class for the validation set\n    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n    label_count = {}\n    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n        label = labels[train_file.split('.')[0]]\n        fname = os.path.join(data_dir, 'train', train_file)\n        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                     'train_valid', label))\n        if label not in label_count or label_count[label] < n_valid_per_label:\n            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                         'valid', label))\n            label_count[label] = label_count.get(label, 0) + 1\n        else:\n            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n                                         'train', label))\n    return n_valid_per_label\n\ndef reorg_test(data_dir):\n    \"\"\"Organize the testing set for data loading during prediction.\"\"\"\n    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n        copyfile(os.path.join(data_dir, 'test', test_file),\n                 os.path.join(data_dir, 'train_valid_test', 'test',\n                              'unknown'))\n        \ndef reorg_cifar10_data(data_dir, valid_ratio):\n    labels = read_csv_labels('/kaggle/input/cifar-10/trainLabels.csv')\n    reorg_train_valid(data_dir, labels, valid_ratio)\n    reorg_test(data_dir)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:48:59.999674Z","iopub.execute_input":"2024-01-22T22:49:00.000049Z","iopub.status.idle":"2024-01-22T22:49:01.151041Z","shell.execute_reply.started":"2024-01-22T22:49:00.000008Z","shell.execute_reply":"2024-01-22T22:49:01.150007Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='CIFAR-10', save_code=True)\nbatch_size = 64\nwandb.log({'Batch_size': batch_size})\nvalid_ratio = 0.1\nreorg_cifar10_data(data_dir, valid_ratio)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:49:01.154854Z","iopub.execute_input":"2024-01-22T22:49:01.155264Z","iopub.status.idle":"2024-01-22T22:50:47.054860Z","shell.execute_reply.started":"2024-01-22T22:49:01.155224Z","shell.execute_reply":"2024-01-22T22:50:47.053805Z"},"trusted":true},"execution_count":88,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:keztks5g) before initializing another..."},"metadata":{}},{"name":"stderr","text":"wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\nwandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.387 MB of 0.480 MB uploaded\\r'), FloatProgress(value=0.8073447586017635, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch_size</td><td>▁</td></tr><tr><td>Epoch</td><td>▁</td></tr><tr><td>LR</td><td>▁</td></tr><tr><td>Training Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch_size</td><td>64</td></tr><tr><td>Epoch</td><td>1</td></tr><tr><td>LR</td><td>0.003</td></tr><tr><td>Training Loss</td><td>1.6023</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fluent-microwave-31</strong> at: <a href='https://wandb.ai/ales-2000-09/CIFAR-10/runs/keztks5g' target=\"_blank\">https://wandb.ai/ales-2000-09/CIFAR-10/runs/keztks5g</a><br/>Synced 6 W&B file(s), 0 media file(s), 16 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240122_224536-keztks5g/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:keztks5g). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240122_224901-ud49vyan</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ales-2000-09/CIFAR-10/runs/ud49vyan' target=\"_blank\">amber-flower-32</a></strong> to <a href='https://wandb.ai/ales-2000-09/CIFAR-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ales-2000-09/CIFAR-10' target=\"_blank\">https://wandb.ai/ales-2000-09/CIFAR-10</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ales-2000-09/CIFAR-10/runs/ud49vyan' target=\"_blank\">https://wandb.ai/ales-2000-09/CIFAR-10/runs/ud49vyan</a>"},"metadata":{}}]},{"cell_type":"code","source":"transform_train = torchvision.transforms.Compose([\n    transforms.RandomResizedCrop(32),\n    transforms.RandomHorizontalFlip(),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                     [0.2023, 0.1994, 0.2010])\n])\n\ntransform_test = torchvision.transforms.Compose([\n    \n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n                                     [0.2023, 0.1994, 0.2010])])","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:50:47.057321Z","iopub.execute_input":"2024-01-22T22:50:47.057632Z","iopub.status.idle":"2024-01-22T22:50:47.063512Z","shell.execute_reply.started":"2024-01-22T22:50:47.057606Z","shell.execute_reply":"2024-01-22T22:50:47.062519Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_train) for folder in ['train', 'train_valid']]\n\nvalid_ds, test_ds = [torchvision.datasets.ImageFolder(\n    os.path.join(data_dir, 'train_valid_test', folder),\n    transform=transform_test) for folder in ['valid', 'test']]\n\ntrain_iter, train_valid_iter = [torch.utils.data.DataLoader(\n    dataset, batch_size, shuffle=True, drop_last=True)\n    for dataset in (train_ds, train_valid_ds)]\n\nvalid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n                                         drop_last=True)\n\ntest_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n                                        drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:50:47.065067Z","iopub.execute_input":"2024-01-22T22:50:47.065438Z","iopub.status.idle":"2024-01-22T22:50:49.073248Z","shell.execute_reply.started":"2024-01-22T22:50:47.065403Z","shell.execute_reply":"2024-01-22T22:50:49.072182Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained ResNet-18 model\nresnet18 = torchvision.models.resnet18(pretrained=True)\n\n# Modify the fully connected layer (fc) for the CIFAR-10 dataset (10 classes)\nresnet18.fc = nn.Linear(resnet18.fc.in_features, 10)\n\n# Initialize the weights of the fully connected layer using Xavier normal initialization\nnn.init.xavier_normal_(resnet18.fc.weight)\n\n# Initialize the bias of the fully connected layer to zeros\nnn.init.constant_(resnet18.fc.bias, 0)\n\n# Set the device to GPU if available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nresnet18 = resnet18.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:50:49.074734Z","iopub.execute_input":"2024-01-22T22:50:49.075116Z","iopub.status.idle":"2024-01-22T22:50:49.296352Z","shell.execute_reply.started":"2024-01-22T22:50:49.075078Z","shell.execute_reply":"2024-01-22T22:50:49.295427Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"lr = 0.003\nwandb.log({'LR': lr})\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.SGD(resnet18.parameters(), lr=lr, momentum=0.9)\n\n# Training loop\nnum_epochs = 32\nfor epoch in range(num_epochs):\n    resnet18.train()\n    total_loss = 0.0  # Initialize total loss for the epoch\n    for inputs, labels in train_iter:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = resnet18(inputs)\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    average_loss = total_loss / len(train_iter)  # Calculate the average loss for the epoch\n    print(f'Finish Epoch: {epoch + 1}, Training Loss: {average_loss}')\n    wandb.log({'Training Loss': average_loss})\n    wandb.log({'Epoch': epoch + 1})\n\n    # Evaluation on the validation set\n    resnet18.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        val_loss = 0.0\n        for inputs, labels in valid_iter:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = resnet18(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n    average_loss2 = val_loss / len(valid_iter)  \n    print(f'Finish Epoch: {epoch + 1}, Validation Loss: {average_loss2}')\n    wandb.log({'Validation Loss': average_loss2})\n\n    accuracy = correct / total\n    print(f'Validation Accuracy: {accuracy}')\n    wandb.log({'Accuracy on validation': accuracy})\n    \nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T22:50:49.297469Z","iopub.execute_input":"2024-01-22T22:50:49.297773Z","iopub.status.idle":"2024-01-22T23:15:05.739528Z","shell.execute_reply.started":"2024-01-22T22:50:49.297747Z","shell.execute_reply":"2024-01-22T23:15:05.738812Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Finish Epoch: 1, Training Loss: 1.5992823978236865\nFinish Epoch: 1, Validation Loss: 1.1701220296896422\nValidation Accuracy: 0.7017227564102564\nFinish Epoch: 2, Training Loss: 1.3400502006495492\nFinish Epoch: 2, Validation Loss: 1.0842253038516412\nValidation Accuracy: 0.742988782051282\nFinish Epoch: 3, Training Loss: 1.268226931139212\nFinish Epoch: 3, Validation Loss: 1.0413500788884285\nValidation Accuracy: 0.7716346153846154\nFinish Epoch: 4, Training Loss: 1.2235089237795107\nFinish Epoch: 4, Validation Loss: 0.9836106430261563\nValidation Accuracy: 0.7926682692307693\nFinish Epoch: 5, Training Loss: 1.20176811247089\nFinish Epoch: 5, Validation Loss: 0.9814935525258383\nValidation Accuracy: 0.7930689102564102\nFinish Epoch: 6, Training Loss: 1.171153789394102\nFinish Epoch: 6, Validation Loss: 0.9684740534195533\nValidation Accuracy: 0.7942708333333334\nFinish Epoch: 7, Training Loss: 1.1534256230546942\nFinish Epoch: 7, Validation Loss: 0.9560337632130353\nValidation Accuracy: 0.8040865384615384\nFinish Epoch: 8, Training Loss: 1.1346972180294617\nFinish Epoch: 8, Validation Loss: 0.9582959642777076\nValidation Accuracy: 0.8026842948717948\nFinish Epoch: 9, Training Loss: 1.1165269448780912\nFinish Epoch: 9, Validation Loss: 0.9305885105560987\nValidation Accuracy: 0.8062900641025641\nFinish Epoch: 10, Training Loss: 1.10272940217516\nFinish Epoch: 10, Validation Loss: 0.9556579421728085\nValidation Accuracy: 0.8104967948717948\nFinish Epoch: 11, Training Loss: 1.0954945123890893\nFinish Epoch: 11, Validation Loss: 0.9206311183098035\nValidation Accuracy: 0.8104967948717948\nFinish Epoch: 12, Training Loss: 1.0893126139430538\nFinish Epoch: 12, Validation Loss: 0.9113129201607827\nValidation Accuracy: 0.8177083333333334\nFinish Epoch: 13, Training Loss: 1.086145360049279\nFinish Epoch: 13, Validation Loss: 0.9046043570225055\nValidation Accuracy: 0.8259214743589743\nFinish Epoch: 14, Training Loss: 1.0731015252015668\nFinish Epoch: 14, Validation Loss: 0.9078786632953546\nValidation Accuracy: 0.8243189102564102\nFinish Epoch: 15, Training Loss: 1.0613212337873739\nFinish Epoch: 15, Validation Loss: 0.8992041043746166\nValidation Accuracy: 0.8257211538461539\nFinish Epoch: 16, Training Loss: 1.0524114787493797\nFinish Epoch: 16, Validation Loss: 0.9076534020595062\nValidation Accuracy: 0.8251201923076923\nFinish Epoch: 17, Training Loss: 1.048718141118968\nFinish Epoch: 17, Validation Loss: 0.906174517594851\nValidation Accuracy: 0.8217147435897436\nFinish Epoch: 18, Training Loss: 1.0483867040229895\nFinish Epoch: 18, Validation Loss: 0.8770895432203244\nValidation Accuracy: 0.8389423076923077\nFinish Epoch: 19, Training Loss: 1.0340425825729478\nFinish Epoch: 19, Validation Loss: 0.9900773977622007\nValidation Accuracy: 0.8118990384615384\nFinish Epoch: 20, Training Loss: 1.0330476736276282\nFinish Epoch: 20, Validation Loss: 0.8954334472998594\nValidation Accuracy: 0.8315304487179487\nFinish Epoch: 21, Training Loss: 1.0311176789603904\nFinish Epoch: 21, Validation Loss: 0.8580961082226191\nValidation Accuracy: 0.8433493589743589\nFinish Epoch: 22, Training Loss: 1.018195563241055\nFinish Epoch: 22, Validation Loss: 0.8552416861057281\nValidation Accuracy: 0.8441506410256411\nFinish Epoch: 23, Training Loss: 1.0103982441116701\nFinish Epoch: 23, Validation Loss: 0.8675618859437796\nValidation Accuracy: 0.8397435897435898\nFinish Epoch: 24, Training Loss: 1.0091158749368758\nFinish Epoch: 24, Validation Loss: 0.856075590237593\nValidation Accuracy: 0.8517628205128205\nFinish Epoch: 25, Training Loss: 1.0018986874759452\nFinish Epoch: 25, Validation Loss: 0.8783799058351761\nValidation Accuracy: 0.8341346153846154\nFinish Epoch: 26, Training Loss: 1.004421449487613\nFinish Epoch: 26, Validation Loss: 0.8548206671690329\nValidation Accuracy: 0.8505608974358975\nFinish Epoch: 27, Training Loss: 0.99346169765779\nFinish Epoch: 27, Validation Loss: 0.8954477302539043\nValidation Accuracy: 0.8335336538461539\nFinish Epoch: 28, Training Loss: 0.9965301126368863\nFinish Epoch: 28, Validation Loss: 0.8819051109827482\nValidation Accuracy: 0.8397435897435898\nFinish Epoch: 29, Training Loss: 0.9892476680099032\nFinish Epoch: 29, Validation Loss: 0.8504597857976571\nValidation Accuracy: 0.852363782051282\nFinish Epoch: 30, Training Loss: 0.9799839742322735\nFinish Epoch: 30, Validation Loss: 0.8564288990619855\nValidation Accuracy: 0.8503605769230769\nFinish Epoch: 31, Training Loss: 0.9813118600743593\nFinish Epoch: 31, Validation Loss: 0.8682496731097882\nValidation Accuracy: 0.8467548076923077\nFinish Epoch: 32, Training Loss: 0.9818410697905811\nFinish Epoch: 32, Validation Loss: 0.8500096178971804\nValidation Accuracy: 0.8503605769230769\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.051 MB uploaded\\r'), FloatProgress(value=0.02598933483019927, max=1.…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on validation</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▆▇██▇█▇█▇▇████</td></tr><tr><td>Batch_size</td><td>▁</td></tr><tr><td>Epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>LR</td><td>▁</td></tr><tr><td>Training Loss</td><td>█▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Loss</td><td>█▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▄▂▁▁▁▁▂▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on validation</td><td>0.85036</td></tr><tr><td>Batch_size</td><td>64</td></tr><tr><td>Epoch</td><td>32</td></tr><tr><td>LR</td><td>0.003</td></tr><tr><td>Training Loss</td><td>0.98184</td></tr><tr><td>Validation Loss</td><td>0.85001</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">amber-flower-32</strong> at: <a href='https://wandb.ai/ales-2000-09/CIFAR-10/runs/ud49vyan' target=\"_blank\">https://wandb.ai/ales-2000-09/CIFAR-10/runs/ud49vyan</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240122_224901-ud49vyan/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"resnet18 = resnet18.to(device)\nresnet18.eval()\npredictions = []  # List to store predicted labels\n\nwith torch.no_grad():\n    for inputs, labels in test_iter:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = resnet18(inputs)\n        predicted = outputs.argmax(dim=1)\n        predictions.extend(predicted.type(torch.int32).cpu().numpy())\n\n# Create submission file\nidentifiers = list(range(1, len(test_ds) + 1))\nidentifiers.sort(key=lambda x: str(x))\nsubmission_df = pd.DataFrame({'id': identifiers, 'label': predictions})\nsubmission_df['label'] = submission_df['label'].apply(lambda x: train_ds.classes[x])\nsubmission_df.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T23:15:05.740535Z","iopub.execute_input":"2024-01-22T23:15:05.740789Z","iopub.status.idle":"2024-01-22T23:17:46.426826Z","shell.execute_reply.started":"2024-01-22T23:15:05.740766Z","shell.execute_reply":"2024-01-22T23:17:46.425921Z"},"trusted":true},"execution_count":93,"outputs":[]}]}