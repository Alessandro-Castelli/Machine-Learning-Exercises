{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https://www.kaggle.com/c/titanic","metadata":{}},{"cell_type":"markdown","source":"Link ex2: \nhttps://www.kaggle.com/code/alessandromajumba/logistic-regression-castelli\n\nhttps://wandb.ai/ales-2000-09/titanic_kaggle","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-11-14T18:55:31.893575Z","iopub.execute_input":"2023-11-14T18:55:31.894077Z","iopub.status.idle":"2023-11-14T18:55:31.907731Z","shell.execute_reply.started":"2023-11-14T18:55:31.894044Z","shell.execute_reply":"2023-11-14T18:55:31.906010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import all the needed library and init Weights and Biases","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Label\")\n\nimport wandb\nwandb.login(key=secret_value_0)\nwandb.init(project='titanic_kaggle')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:54:20.420269Z","iopub.execute_input":"2023-11-14T18:54:20.420775Z","iopub.status.idle":"2023-11-14T18:55:00.606865Z","shell.execute_reply.started":"2023-11-14T18:54:20.420742Z","shell.execute_reply":"2023-11-14T18:55:00.606057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first need to read the datasets","metadata":{}},{"cell_type":"code","source":"titanic_training_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntitanic_test_data = pd.read_csv('/kaggle/input/titanic/test.csv')\ntitanic_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:34.896803Z","iopub.execute_input":"2023-11-14T18:55:34.897227Z","iopub.status.idle":"2023-11-14T18:55:34.930187Z","shell.execute_reply.started":"2023-11-14T18:55:34.897193Z","shell.execute_reply":"2023-11-14T18:55:34.928700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataframe needs to be cleaned, knowing if some informations are unknown can be very important to determine if someone survived","metadata":{}},{"cell_type":"code","source":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1)\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\nlabels = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\ntitanic_training_data = clean_titanic(titanic_training_data)\ntitanic_training_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:35.209191Z","iopub.execute_input":"2023-11-14T18:55:35.210483Z","iopub.status.idle":"2023-11-14T18:55:35.257523Z","shell.execute_reply.started":"2023-11-14T18:55:35.210437Z","shell.execute_reply":"2023-11-14T18:55:35.255150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","metadata":{}},{"cell_type":"code","source":"titanic_data_tensor = torch.tensor(titanic_training_data.astype('float').values, dtype=torch.float32)\ntitanic_data_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:35.505267Z","iopub.execute_input":"2023-11-14T18:55:35.505704Z","iopub.status.idle":"2023-11-14T18:55:35.517131Z","shell.execute_reply.started":"2023-11-14T18:55:35.505669Z","shell.execute_reply":"2023-11-14T18:55:35.515760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a `TensorDataset` to get tuple of data and label","metadata":{}},{"cell_type":"code","source":"dataset = torch.utils.data.TensorDataset(titanic_data_tensor, labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:35.783548Z","iopub.execute_input":"2023-11-14T18:55:35.783972Z","iopub.status.idle":"2023-11-14T18:55:35.791856Z","shell.execute_reply.started":"2023-11-14T18:55:35.783938Z","shell.execute_reply":"2023-11-14T18:55:35.790203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then split between the training and validation set","metadata":{}},{"cell_type":"code","source":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:35.996419Z","iopub.execute_input":"2023-11-14T18:55:35.996879Z","iopub.status.idle":"2023-11-14T18:55:36.006854Z","shell.execute_reply.started":"2023-11-14T18:55:35.996845Z","shell.execute_reply":"2023-11-14T18:55:36.004640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Layer initialization using Xavier Uniform on the weight and a constant 0 value on the bias","metadata":{}},{"cell_type":"markdown","source":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","metadata":{}},{"cell_type":"code","source":"class LinearModel(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearModel, self).__init__()\n        # Define a linear layer\n        self.linear = nn.Linear(input_size, output_size)\n        # Initialize weights with Xavier Uniform\n        nn.init.xavier_uniform_(self.linear.weight)\n        # Set bias to 0\n        nn.init.constant_(self.linear.bias, 0)\n    \n    def forward(self, x):\n        # Apply the linear transformation\n        x = self.linear(x)\n        # Apply the sigmoid activation function\n        x = torch.sigmoid(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:36.383671Z","iopub.execute_input":"2023-11-14T18:55:36.384359Z","iopub.status.idle":"2023-11-14T18:55:36.393287Z","shell.execute_reply.started":"2023-11-14T18:55:36.384309Z","shell.execute_reply":"2023-11-14T18:55:36.391829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initialize the network (call it `net`, it would makes things easier later), the loss, the optimizer and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Variable\n\nnum_epochs = 400 # should be more than enought, but can be changed\nlr = 3e-3 # e.q to 0.003, you can change it if needed\n\ninput_size = titanic_training_data.shape[1]\noutput_size = 1\nnet = LinearModel(input_size, output_size)\n\ncriterion = nn.BCELoss() # I use Binary Cross Entropy Loss as the loss function\n\noptimizer = torch.optim.RMSprop(net.parameters(), lr = lr)\n\nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TRAINING LOOP\n    for batch in data_loader_train:\n        X = Variable(torch.FloatTensor(batch[0]))\n        y = Variable(torch.FloatTensor(batch[1]))\n        \n        optimizer.zero_grad()\n        outputs = net(X).reshape(X.shape[0]).float()\n        loss = criterion(outputs, y)\n        training_loss += loss\n        loss.sum().backward()\n        optimizer.step()\n    validation_loss = 0\n    #VALIDATION LOOP\n    with torch.no_grad():\n        for batch in data_loader_val:\n            X = batch[0]\n            y = batch[1]\n            \n            \n            predictions = net(X).reshape(X.shape[0]).float()\n            loss = criterion(predictions, y)\n            validation_loss += loss\n            \n    wandb.log({'training_loss': (training_loss/64).item(), 'validation_loss': (validation_loss/64).item()})\n    wandb.log({'Net': net.state_dict()})\n    print({'epoch':(epoch), 'training_loss': (training_loss/64).item(), 'validation_loss': (validation_loss/64).item()})\n    # SAVE THE MODEL\n    PATH = \"./mymodel.pth\"\n    torch.save(net.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:37.126950Z","iopub.execute_input":"2023-11-14T18:55:37.127370Z","iopub.status.idle":"2023-11-14T18:55:45.081635Z","shell.execute_reply.started":"2023-11-14T18:55:37.127332Z","shell.execute_reply":"2023-11-14T18:55:45.080491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This loop computes the prediction on the test dataset and create a submission file\n\nYou then just have to click the submit button to get your score, lucky you!","metadata":{}},{"cell_type":"code","source":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T18:55:45.083999Z","iopub.execute_input":"2023-11-14T18:55:45.084442Z","iopub.status.idle":"2023-11-14T18:55:45.141572Z","shell.execute_reply.started":"2023-11-14T18:55:45.084400Z","shell.execute_reply":"2023-11-14T18:55:45.140224Z"},"trusted":true},"execution_count":null,"outputs":[]}]}