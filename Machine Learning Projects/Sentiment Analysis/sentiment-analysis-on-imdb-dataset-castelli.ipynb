{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMDB sentiment analysis with RNNs\n\n## Kaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n\n## Alessandro Castelli Kaggle Code: https://www.kaggle.com/code/alessandromajumba/sentiment-analysis-on-imdb-dataset-castelli\n\n## Alessandro Castelli WANDB: https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews?workspace=user-ales-2000-09","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom spellchecker import SpellChecker\nfrom tqdm import tqdm\n# allows to have a progress bar in pandas, useful for long processing operations\ntqdm.pandas()\nfrom collections import Counter\nimport torch\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\n# !pip install --upgrade pandas numpy nltk scikit-learn pyspellchecker tqdm torch","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:44.784479Z","iopub.execute_input":"2024-01-29T18:18:44.784883Z","iopub.status.idle":"2024-01-29T18:18:44.792025Z","shell.execute_reply.started":"2024-01-29T18:18:44.784851Z","shell.execute_reply":"2024-01-29T18:18:44.791007Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"Read the dataset and observe the first 5 rows.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:44.793636Z","iopub.execute_input":"2024-01-29T18:18:44.793967Z","iopub.status.idle":"2024-01-29T18:18:45.401221Z","shell.execute_reply.started":"2024-01-29T18:18:44.793942Z","shell.execute_reply":"2024-01-29T18:18:45.400244Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Lucky us, the dataset is well-balanced.","metadata":{}},{"cell_type":"code","source":"data.sentiment.value_counts()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:45.402939Z","iopub.execute_input":"2024-01-29T18:18:45.403231Z","iopub.status.idle":"2024-01-29T18:18:45.416553Z","shell.execute_reply.started":"2024-01-29T18:18:45.403206Z","shell.execute_reply":"2024-01-29T18:18:45.415551Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"sentiment\npositive    25000\nnegative    25000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Transform the labels to 0 and 1.","metadata":{}},{"cell_type":"code","source":"def transform_label(label):\n    return 1 if label == 'positive' else 0\n\n\ndata['label'] = data['sentiment'].progress_apply(transform_label)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:45.417705Z","iopub.execute_input":"2024-01-29T18:18:45.417991Z","iopub.status.idle":"2024-01-29T18:18:45.535519Z","shell.execute_reply.started":"2024-01-29T18:18:45.417967Z","shell.execute_reply":"2024-01-29T18:18:45.534645Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [00:00<00:00, 466871.03it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing\n\n- In classic NLP, the text is often preprocessed to remove tokens that might confuse the classifier\n- Below you can find some examples of possible preprocessing techniques\n- Feel free to modify them to improve the results of your classifier","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('omw-1.4')\nstopwords = set(stopwords.words('english'))\n\ndef rm_link(text):\n    return re.sub(r'http\\S+', '', text)\n\n\n# handle case like \"shut up okay?Im only 10 years old\"\n# become \"shut up okay Im only 10 years old\"\ndef rm_punct2(text):\n    return re.sub(r'[\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n    # return re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n\n\ndef rm_html(text):\n    # remove html tags\n    text = re.sub(r'<.*?>', '', text)\n    # remove <br /> tags\n    return re.sub(r'<br />', '', text)\n\n\ndef space_bt_punct(text):\n    pattern = r'([.,!?-])'\n    s = re.sub(pattern, r' \\1 ', text)  # add whitespaces between punctuation\n    s = re.sub(r'\\s{2,}', ' ', s)  # remove double whitespaces\n    return s\n\n\ndef rm_number(text):\n    return re.sub(r'\\d+', '', text)\n\n\ndef rm_whitespaces(text):\n    return re.sub(r'\\s+', ' ', text)\n\n\ndef rm_nonascii(text):\n    return re.sub(r'[^\\x00-\\x7f]', r'', text)\n\n\ndef rm_emoji(text):\n    emojis = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE\n    )\n    return emojis.sub(r'', text)\n\n\ndef spell_correction(text):\n    # if too slow: return text\n    return text\n    # https://pypi.org/project/pyspellchecker/\n    spell = SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            candidate = spell.correction(word)\n            if candidate is not None:\n                corrected_text.append(candidate)\n            else:\n                corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return ' '.join(corrected_text)\n\n\ndef clean_pipeline(text):\n    text = text.lower()\n    no_link = rm_link(text)\n    no_html = rm_html(no_link)\n    space_punct = space_bt_punct(no_html)\n    no_punct = rm_punct2(space_punct)\n    no_number = rm_number(no_punct)\n    no_whitespaces = rm_whitespaces(no_number)\n    no_nonasci = rm_nonascii(no_whitespaces)\n    no_emoji = rm_emoji(no_nonasci)\n    spell_corrected = spell_correction(no_emoji)\n    return spell_corrected","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:45.538364Z","iopub.execute_input":"2024-01-29T18:18:45.538790Z","iopub.status.idle":"2024-01-29T18:18:45.553819Z","shell.execute_reply.started":"2024-01-29T18:18:45.538764Z","shell.execute_reply":"2024-01-29T18:18:45.552827Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's clean the reviews first:","metadata":{}},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(clean_pipeline)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:18:45.554945Z","iopub.execute_input":"2024-01-29T18:18:45.555290Z","iopub.status.idle":"2024-01-29T18:19:06.911953Z","shell.execute_reply.started":"2024-01-29T18:18:45.555246Z","shell.execute_reply":"2024-01-29T18:19:06.911064Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [00:21<00:00, 2342.64it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We now tokenize and remove stopwords (i.e. the, a, an, etc.) and lemmatize the words (i.e. running -> run, better -> good, etc.).","metadata":{}},{"cell_type":"code","source":"# preprocessing\ndef tokenize(text):\n    return word_tokenize(text)\n\n\ndef rm_stopwords(text):\n    return [i for i in text if i not in stopwords]\n\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmas = [lemmatizer.lemmatize(t) for t in text]\n    # make sure lemmas does not contains stopwords\n    return rm_stopwords(lemmas)\n\n\ndef preprocess_pipeline(text):\n    tokens = tokenize(text)\n    no_stopwords = rm_stopwords(tokens)\n    lemmas = lemmatize(no_stopwords)\n    return ' '.join(lemmas)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:19:06.913381Z","iopub.execute_input":"2024-01-29T18:19:06.913763Z","iopub.status.idle":"2024-01-29T18:19:06.921327Z","shell.execute_reply.started":"2024-01-29T18:19:06.913727Z","shell.execute_reply":"2024-01-29T18:19:06.920407Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"data['review'] = data['review'].progress_apply(preprocess_pipeline)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:19:06.922613Z","iopub.execute_input":"2024-01-29T18:19:06.922962Z","iopub.status.idle":"2024-01-29T18:21:02.617921Z","shell.execute_reply.started":"2024-01-29T18:19:06.922929Z","shell.execute_reply":"2024-01-29T18:21:02.616980Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stderr","text":"100%|██████████| 50000/50000 [01:55<00:00, 432.25it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's check the result.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:02.619158Z","iopub.execute_input":"2024-01-29T18:21:02.619468Z","iopub.status.idle":"2024-01-29T18:21:02.629152Z","shell.execute_reply.started":"2024-01-29T18:21:02.619441Z","shell.execute_reply":"2024-01-29T18:21:02.628327Z"},"trusted":true},"execution_count":87,"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  label\n0  one reviewer mentioned watching oz episode hoo...  positive      1\n1  wonderful little production filming technique ...  positive      1\n2  thought wonderful way spend time hot summer we...  positive      1\n3  basically family little boy jake think zombie ...  negative      0\n4  petter mattei love time money visually stunnin...  positive      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one reviewer mentioned watching oz episode hoo...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonderful little production filming technique ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thought wonderful way spend time hot summer we...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically family little boy jake think zombie ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei love time money visually stunnin...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Embedding\n\n- ANNs cannot process text input\n- Input tokens must be mapped to integers using a vocabulary\n- In this example, we build a vocabulary manually, but you can also replace this code with an [embedding layer](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)","metadata":{}},{"cell_type":"code","source":"# get all processed reviews\nreviews = data.review.values\n# merge into single variable, separated by whitespaces\nwords = ' '.join(reviews)\n# obtain list of words\nwords = words.split()\n# build vocabulary\ncounter = Counter(words)\n# only keep top 2000 words\nvocab = sorted(counter, key=counter.get, reverse=True)[:2000]\nint2word = dict(enumerate(vocab, 2))\nint2word[0] = '<PAD>'\nint2word[1] = '<UNK>'\nword2int = {word: id for id, word in int2word.items()}","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:02.630535Z","iopub.execute_input":"2024-01-29T18:21:02.630979Z","iopub.status.idle":"2024-01-29T18:21:03.931207Z","shell.execute_reply.started":"2024-01-29T18:21:02.630943Z","shell.execute_reply":"2024-01-29T18:21:03.930297Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"reviews_enc = [[word2int[word] if word in word2int else word2int['<UNK>'] for word in review.split()] for review in tqdm(reviews, desc='encoding')]","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:03.934513Z","iopub.execute_input":"2024-01-29T18:21:03.934789Z","iopub.status.idle":"2024-01-29T18:21:05.882459Z","shell.execute_reply.started":"2024-01-29T18:21:03.934766Z","shell.execute_reply":"2024-01-29T18:21:05.881523Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stderr","text":"encoding: 100%|██████████| 50000/50000 [00:01<00:00, 26242.55it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Because we have to build batch, we have to pad the reviews to the same length. We will pad the reviews with <PAD> token.\n**Because we use RNNs, we need to left pad and not right pad the sequence.**","metadata":{}},{"cell_type":"code","source":"# left padding sequences\ndef pad_features(reviews, pad_id, seq_length=128):\n    # features = np.zeros((len(reviews), seq_length), dtype=int)\n    features = np.full((len(reviews), seq_length), pad_id, dtype=int)\n\n    for i, row in enumerate(reviews):\n        start_index = max(0, seq_length - len(row))\n        # if seq_length < len(row) then review will be trimmed\n        features[i, start_index:] = np.array(row)[:min(seq_length, len(row))]\n\n    return features\n\n\nseq_length = 128\nfeatures = pad_features(reviews_enc, pad_id=word2int['<PAD>'], seq_length=seq_length)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:05.884780Z","iopub.execute_input":"2024-01-29T18:21:05.885167Z","iopub.status.idle":"2024-01-29T18:21:06.552940Z","shell.execute_reply.started":"2024-01-29T18:21:05.885128Z","shell.execute_reply":"2024-01-29T18:21:06.552095Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"## Split the data","metadata":{}},{"cell_type":"code","source":"labels = data.label.to_numpy()\n\n# train test split\ntrain_size = .75  # we will use 75% of whole data as train set\nval_size = .5  # and we will use 50% of test set as validation set\n\n# stratify will make sure that train and test set have same distribution of labels\ntrain_x, test_x, train_y, test_y = train_test_split(features, labels, test_size=1 - train_size, stratify=labels)\n\n# split test set into validation and test set\nval_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=val_size, stratify=test_y)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:06.554114Z","iopub.execute_input":"2024-01-29T18:21:06.554425Z","iopub.status.idle":"2024-01-29T18:21:06.613215Z","shell.execute_reply.started":"2024-01-29T18:21:06.554398Z","shell.execute_reply":"2024-01-29T18:21:06.612436Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Label\")\n\nimport wandb\nwandb.login(key=secret_value_0)\nwandb.init(project='IMDB Dataset of 50K Movie Reviews', save_code=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T18:21:06.614331Z","iopub.execute_input":"2024-01-29T18:21:06.614635Z","iopub.status.idle":"2024-01-29T18:21:37.954424Z","shell.execute_reply.started":"2024-01-29T18:21:06.614608Z","shell.execute_reply":"2024-01-29T18:21:37.953520Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240129_182106-p93ykrxb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb' target=\"_blank\">bright-water-26</a></strong> to <a href='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews' target=\"_blank\">https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb' target=\"_blank\">https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb</a>"},"metadata":{}},{"execution_count":92,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ac3803dd0c0>"},"metadata":{}}]},{"cell_type":"markdown","source":"Define the datasets and dataloaders.","metadata":{}},{"cell_type":"code","source":"# define batch size\nbatch_size = 128\n\n# create tensor datasets\ntrain_dataset = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_dataset = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_dataset = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# create dataloaders\ntrain_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:37.955681Z","iopub.execute_input":"2024-01-29T18:21:37.957659Z","iopub.status.idle":"2024-01-29T18:21:38.333429Z","shell.execute_reply.started":"2024-01-29T18:21:37.957620Z","shell.execute_reply":"2024-01-29T18:21:38.332499Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"Define the model.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers=2):\n        # Initialize the RNN model with specified parameters\n        super(RNN, self).__init__()\n        # Embedding layer to convert input indices to dense vectors\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        # RNN layer with specified parameters\n        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=num_layers, batch_first=True)\n        # Fully connected layer \n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # Apply the embedding layer to convert input indices to dense vectors\n        x = self.embedding(x)\n        # Pass the embedded input through the RNN layer\n        rnn_out, h_n = self.rnn(x)\n        # Assuming h_n is a tuple of hidden states from all layers\n        # Concatenate the hidden states from all layers (assuming the last layer [-1])\n        h_n = h_n[-1].squeeze(0)\n        # Pass the concatenated hidden states through the fully connected layer\n        output = self.fc(h_n)\n        return output\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:38.334751Z","iopub.execute_input":"2024-01-29T18:21:38.335104Z","iopub.status.idle":"2024-01-29T18:21:38.741868Z","shell.execute_reply.started":"2024-01-29T18:21:38.335071Z","shell.execute_reply":"2024-01-29T18:21:38.740955Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"Instantiate the model.","metadata":{}},{"cell_type":"code","source":"# Define the layer dimensions\nvocab_size = len(word2int)  # The number of unique words in your vocabulary\nembed_size = 256  # Dimension of the word embeddings\nhidden_size = 128  # Number of features in the hidden state\noutput_size = 1  # Dimension of the output, e.g., for a binary classification problem\n\n# Configuration for WandB\nconfig = wandb.config\nconfig.vocab_size = vocab_size  # Number of words in the vocabulary\nconfig.embed_size = embed_size  # Dimension of the word embeddings\nconfig.hidden_size = hidden_size  # Dimension of the hidden state\nconfig.output_size = output_size  # Dimension of the output\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:38.743045Z","iopub.execute_input":"2024-01-29T18:21:38.743362Z","iopub.status.idle":"2024-01-29T18:21:39.142423Z","shell.execute_reply.started":"2024-01-29T18:21:38.743334Z","shell.execute_reply":"2024-01-29T18:21:39.141494Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the model\nmodel = RNN(vocab_size, embed_size, hidden_size, output_size, num_layers=3)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nif device == 'cuda':\n     model = torch.nn.DataParallel(model)\n        \nmodel.to(device)\n# Print the structure of the model\nprint(model)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:39.145038Z","iopub.execute_input":"2024-01-29T18:21:39.145379Z","iopub.status.idle":"2024-01-29T18:21:39.525295Z","shell.execute_reply.started":"2024-01-29T18:21:39.145347Z","shell.execute_reply":"2024-01-29T18:21:39.524437Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): RNN(\n    (embedding): Embedding(2002, 256)\n    (rnn): RNN(256, 128, num_layers=3, batch_first=True)\n    (fc): Linear(in_features=128, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Define the loss function and optimizer.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define the loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Define the optimizer\nlr = 0.000005\nwandb.log({\"LR\": lr})\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n#optimizer = optim.RMSprop(model.parameters(), lr=lr, alpha=0.9)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:39.526362Z","iopub.execute_input":"2024-01-29T18:21:39.526625Z","iopub.status.idle":"2024-01-29T18:21:39.895348Z","shell.execute_reply.started":"2024-01-29T18:21:39.526600Z","shell.execute_reply":"2024-01-29T18:21:39.894424Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"Define the training loop.","metadata":{}},{"cell_type":"code","source":"# Number of epochs\nnum_epochs = 256\n\nfor epoch in range(num_epochs):\n    # Set the model to training mode\n    model.train()\n    \n    # Variable for total loss in each epoch\n    total_loss = 0.0\n    \n    # Iterate through the training data\n    for inputs, labels in train_loader:\n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the labels\n        labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n            \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update the total loss\n        total_loss += loss.item()\n    \n    # Calculate the average loss per epoch\n    Training_Loss = total_loss / len(train_loader)\n    \n    # Print the average loss per epoch during training\n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {Training_Loss:.4f}')\n    \n    # Set the model to evaluation mode\n    model.eval()\n    \n    # Variables for total loss and number of correct predictions\n    total_loss = 0.0\n    correct_predictions = 0\n\n    # Iterate through the validation data\n    with torch.no_grad():  # Disable gradient computation during evaluation\n        for inputs, labels in valid_loader:\n            \n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(inputs)\n            \n            # Reshape the labels\n            labels = labels.view(-1, 1)  # Change dimensions to [batch_size, 1]\n                \n            # Compute the loss\n            loss = criterion(outputs, labels.float())  # Convert labels to float\n            \n            # Update the total loss\n            total_loss += loss.item()\n            \n            # Calculate the number of correct predictions\n            threshold = 0.5\n            predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n            correct_predictions += (predicted_labels == labels.float()).sum().item()\n\n    \n    # Calculate the average loss per epoch during evaluation\n    average_loss = total_loss / len(valid_loader)\n    \n    # Calculate accuracy\n    accuracy = correct_predictions / len(valid_loader.dataset)\n    \n    # Print evaluation metrics\n    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n    \n    # Log metrics using WandB\n    wandb.log({\"Epoch\": epoch+1,\"Training Loss\": Training_Loss, \"Validation Loss\": average_loss, \"Validation Accuracy\": accuracy})\n\n# Print a completion message\nprint('Training and Validation completed!')","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:21:39.896538Z","iopub.execute_input":"2024-01-29T18:21:39.896894Z","iopub.status.idle":"2024-01-29T18:33:45.665901Z","shell.execute_reply.started":"2024-01-29T18:21:39.896845Z","shell.execute_reply":"2024-01-29T18:33:45.664852Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Epoch [1/256], Training Loss: 0.6951\nEpoch [1/256], Validation Loss: 0.6954, Validation Accuracy: 0.5022\nEpoch [2/256], Training Loss: 0.6926\nEpoch [2/256], Validation Loss: 0.6936, Validation Accuracy: 0.4965\nEpoch [3/256], Training Loss: 0.6908\nEpoch [3/256], Validation Loss: 0.6921, Validation Accuracy: 0.5154\nEpoch [4/256], Training Loss: 0.6890\nEpoch [4/256], Validation Loss: 0.6907, Validation Accuracy: 0.5288\nEpoch [5/256], Training Loss: 0.6872\nEpoch [5/256], Validation Loss: 0.6893, Validation Accuracy: 0.5344\nEpoch [6/256], Training Loss: 0.6851\nEpoch [6/256], Validation Loss: 0.6875, Validation Accuracy: 0.5432\nEpoch [7/256], Training Loss: 0.6826\nEpoch [7/256], Validation Loss: 0.6853, Validation Accuracy: 0.5491\nEpoch [8/256], Training Loss: 0.6793\nEpoch [8/256], Validation Loss: 0.6823, Validation Accuracy: 0.5586\nEpoch [9/256], Training Loss: 0.6749\nEpoch [9/256], Validation Loss: 0.6783, Validation Accuracy: 0.5669\nEpoch [10/256], Training Loss: 0.6683\nEpoch [10/256], Validation Loss: 0.6722, Validation Accuracy: 0.5763\nEpoch [11/256], Training Loss: 0.6581\nEpoch [11/256], Validation Loss: 0.6611, Validation Accuracy: 0.5933\nEpoch [12/256], Training Loss: 0.6386\nEpoch [12/256], Validation Loss: 0.6319, Validation Accuracy: 0.6402\nEpoch [13/256], Training Loss: 0.6062\nEpoch [13/256], Validation Loss: 0.6058, Validation Accuracy: 0.6763\nEpoch [14/256], Training Loss: 0.5906\nEpoch [14/256], Validation Loss: 0.5956, Validation Accuracy: 0.6861\nEpoch [15/256], Training Loss: 0.5804\nEpoch [15/256], Validation Loss: 0.5894, Validation Accuracy: 0.6920\nEpoch [16/256], Training Loss: 0.5773\nEpoch [16/256], Validation Loss: 0.5876, Validation Accuracy: 0.6909\nEpoch [17/256], Training Loss: 0.5690\nEpoch [17/256], Validation Loss: 0.5795, Validation Accuracy: 0.6984\nEpoch [18/256], Training Loss: 0.5623\nEpoch [18/256], Validation Loss: 0.5750, Validation Accuracy: 0.7053\nEpoch [19/256], Training Loss: 0.5560\nEpoch [19/256], Validation Loss: 0.5656, Validation Accuracy: 0.7146\nEpoch [20/256], Training Loss: 0.5506\nEpoch [20/256], Validation Loss: 0.5629, Validation Accuracy: 0.7187\nEpoch [21/256], Training Loss: 0.5454\nEpoch [21/256], Validation Loss: 0.5609, Validation Accuracy: 0.7182\nEpoch [22/256], Training Loss: 0.5415\nEpoch [22/256], Validation Loss: 0.5565, Validation Accuracy: 0.7226\nEpoch [23/256], Training Loss: 0.5374\nEpoch [23/256], Validation Loss: 0.5623, Validation Accuracy: 0.7218\nEpoch [24/256], Training Loss: 0.5340\nEpoch [24/256], Validation Loss: 0.5496, Validation Accuracy: 0.7299\nEpoch [25/256], Training Loss: 0.5300\nEpoch [25/256], Validation Loss: 0.5502, Validation Accuracy: 0.7290\nEpoch [26/256], Training Loss: 0.5271\nEpoch [26/256], Validation Loss: 0.5439, Validation Accuracy: 0.7341\nEpoch [27/256], Training Loss: 0.5231\nEpoch [27/256], Validation Loss: 0.5398, Validation Accuracy: 0.7357\nEpoch [28/256], Training Loss: 0.5201\nEpoch [28/256], Validation Loss: 0.5360, Validation Accuracy: 0.7400\nEpoch [29/256], Training Loss: 0.5164\nEpoch [29/256], Validation Loss: 0.5356, Validation Accuracy: 0.7413\nEpoch [30/256], Training Loss: 0.5137\nEpoch [30/256], Validation Loss: 0.5318, Validation Accuracy: 0.7456\nEpoch [31/256], Training Loss: 0.5102\nEpoch [31/256], Validation Loss: 0.5299, Validation Accuracy: 0.7458\nEpoch [32/256], Training Loss: 0.5069\nEpoch [32/256], Validation Loss: 0.5277, Validation Accuracy: 0.7446\nEpoch [33/256], Training Loss: 0.5043\nEpoch [33/256], Validation Loss: 0.5230, Validation Accuracy: 0.7539\nEpoch [34/256], Training Loss: 0.5008\nEpoch [34/256], Validation Loss: 0.5202, Validation Accuracy: 0.7538\nEpoch [35/256], Training Loss: 0.4982\nEpoch [35/256], Validation Loss: 0.5184, Validation Accuracy: 0.7534\nEpoch [36/256], Training Loss: 0.4955\nEpoch [36/256], Validation Loss: 0.5152, Validation Accuracy: 0.7578\nEpoch [37/256], Training Loss: 0.4931\nEpoch [37/256], Validation Loss: 0.5137, Validation Accuracy: 0.7566\nEpoch [38/256], Training Loss: 0.4907\nEpoch [38/256], Validation Loss: 0.5117, Validation Accuracy: 0.7611\nEpoch [39/256], Training Loss: 0.4878\nEpoch [39/256], Validation Loss: 0.5077, Validation Accuracy: 0.7629\nEpoch [40/256], Training Loss: 0.4849\nEpoch [40/256], Validation Loss: 0.5068, Validation Accuracy: 0.7634\nEpoch [41/256], Training Loss: 0.4825\nEpoch [41/256], Validation Loss: 0.5034, Validation Accuracy: 0.7667\nEpoch [42/256], Training Loss: 0.4797\nEpoch [42/256], Validation Loss: 0.5024, Validation Accuracy: 0.7683\nEpoch [43/256], Training Loss: 0.4774\nEpoch [43/256], Validation Loss: 0.4992, Validation Accuracy: 0.7661\nEpoch [44/256], Training Loss: 0.4760\nEpoch [44/256], Validation Loss: 0.4976, Validation Accuracy: 0.7666\nEpoch [45/256], Training Loss: 0.4730\nEpoch [45/256], Validation Loss: 0.4957, Validation Accuracy: 0.7683\nEpoch [46/256], Training Loss: 0.4703\nEpoch [46/256], Validation Loss: 0.4944, Validation Accuracy: 0.7688\nEpoch [47/256], Training Loss: 0.4681\nEpoch [47/256], Validation Loss: 0.4903, Validation Accuracy: 0.7706\nEpoch [48/256], Training Loss: 0.4660\nEpoch [48/256], Validation Loss: 0.4912, Validation Accuracy: 0.7682\nEpoch [49/256], Training Loss: 0.4635\nEpoch [49/256], Validation Loss: 0.4869, Validation Accuracy: 0.7739\nEpoch [50/256], Training Loss: 0.4612\nEpoch [50/256], Validation Loss: 0.4871, Validation Accuracy: 0.7773\nEpoch [51/256], Training Loss: 0.4597\nEpoch [51/256], Validation Loss: 0.4833, Validation Accuracy: 0.7752\nEpoch [52/256], Training Loss: 0.4573\nEpoch [52/256], Validation Loss: 0.4808, Validation Accuracy: 0.7773\nEpoch [53/256], Training Loss: 0.4549\nEpoch [53/256], Validation Loss: 0.4800, Validation Accuracy: 0.7762\nEpoch [54/256], Training Loss: 0.4531\nEpoch [54/256], Validation Loss: 0.4790, Validation Accuracy: 0.7776\nEpoch [55/256], Training Loss: 0.4507\nEpoch [55/256], Validation Loss: 0.4797, Validation Accuracy: 0.7760\nEpoch [56/256], Training Loss: 0.4490\nEpoch [56/256], Validation Loss: 0.4786, Validation Accuracy: 0.7773\nEpoch [57/256], Training Loss: 0.4477\nEpoch [57/256], Validation Loss: 0.4733, Validation Accuracy: 0.7818\nEpoch [58/256], Training Loss: 0.4468\nEpoch [58/256], Validation Loss: 0.4753, Validation Accuracy: 0.7784\nEpoch [59/256], Training Loss: 0.4439\nEpoch [59/256], Validation Loss: 0.4712, Validation Accuracy: 0.7858\nEpoch [60/256], Training Loss: 0.4412\nEpoch [60/256], Validation Loss: 0.4693, Validation Accuracy: 0.7842\nEpoch [61/256], Training Loss: 0.4402\nEpoch [61/256], Validation Loss: 0.4675, Validation Accuracy: 0.7861\nEpoch [62/256], Training Loss: 0.4384\nEpoch [62/256], Validation Loss: 0.4661, Validation Accuracy: 0.7869\nEpoch [63/256], Training Loss: 0.4359\nEpoch [63/256], Validation Loss: 0.4781, Validation Accuracy: 0.7750\nEpoch [64/256], Training Loss: 0.4352\nEpoch [64/256], Validation Loss: 0.4642, Validation Accuracy: 0.7893\nEpoch [65/256], Training Loss: 0.4336\nEpoch [65/256], Validation Loss: 0.4640, Validation Accuracy: 0.7858\nEpoch [66/256], Training Loss: 0.4306\nEpoch [66/256], Validation Loss: 0.4606, Validation Accuracy: 0.7893\nEpoch [67/256], Training Loss: 0.4286\nEpoch [67/256], Validation Loss: 0.4612, Validation Accuracy: 0.7902\nEpoch [68/256], Training Loss: 0.4272\nEpoch [68/256], Validation Loss: 0.4588, Validation Accuracy: 0.7888\nEpoch [69/256], Training Loss: 0.4256\nEpoch [69/256], Validation Loss: 0.4596, Validation Accuracy: 0.7875\nEpoch [70/256], Training Loss: 0.4248\nEpoch [70/256], Validation Loss: 0.4570, Validation Accuracy: 0.7899\nEpoch [71/256], Training Loss: 0.4225\nEpoch [71/256], Validation Loss: 0.4570, Validation Accuracy: 0.7938\nEpoch [72/256], Training Loss: 0.4213\nEpoch [72/256], Validation Loss: 0.4541, Validation Accuracy: 0.7914\nEpoch [73/256], Training Loss: 0.4197\nEpoch [73/256], Validation Loss: 0.4542, Validation Accuracy: 0.7896\nEpoch [74/256], Training Loss: 0.4178\nEpoch [74/256], Validation Loss: 0.4550, Validation Accuracy: 0.7893\nEpoch [75/256], Training Loss: 0.4176\nEpoch [75/256], Validation Loss: 0.4550, Validation Accuracy: 0.7885\nEpoch [76/256], Training Loss: 0.4161\nEpoch [76/256], Validation Loss: 0.4483, Validation Accuracy: 0.7973\nEpoch [77/256], Training Loss: 0.4129\nEpoch [77/256], Validation Loss: 0.4485, Validation Accuracy: 0.7952\nEpoch [78/256], Training Loss: 0.4120\nEpoch [78/256], Validation Loss: 0.4471, Validation Accuracy: 0.7963\nEpoch [79/256], Training Loss: 0.4097\nEpoch [79/256], Validation Loss: 0.4469, Validation Accuracy: 0.7973\nEpoch [80/256], Training Loss: 0.4096\nEpoch [80/256], Validation Loss: 0.4452, Validation Accuracy: 0.7987\nEpoch [81/256], Training Loss: 0.4075\nEpoch [81/256], Validation Loss: 0.4470, Validation Accuracy: 0.7947\nEpoch [82/256], Training Loss: 0.4066\nEpoch [82/256], Validation Loss: 0.4437, Validation Accuracy: 0.8005\nEpoch [83/256], Training Loss: 0.4046\nEpoch [83/256], Validation Loss: 0.4427, Validation Accuracy: 0.8002\nEpoch [84/256], Training Loss: 0.4034\nEpoch [84/256], Validation Loss: 0.4454, Validation Accuracy: 0.7957\nEpoch [85/256], Training Loss: 0.4012\nEpoch [85/256], Validation Loss: 0.4420, Validation Accuracy: 0.8021\nEpoch [86/256], Training Loss: 0.4005\nEpoch [86/256], Validation Loss: 0.4473, Validation Accuracy: 0.7952\nEpoch [87/256], Training Loss: 0.3992\nEpoch [87/256], Validation Loss: 0.4377, Validation Accuracy: 0.8038\nEpoch [88/256], Training Loss: 0.3985\nEpoch [88/256], Validation Loss: 0.4413, Validation Accuracy: 0.7966\nEpoch [89/256], Training Loss: 0.3971\nEpoch [89/256], Validation Loss: 0.4371, Validation Accuracy: 0.8027\nEpoch [90/256], Training Loss: 0.3951\nEpoch [90/256], Validation Loss: 0.4350, Validation Accuracy: 0.8029\nEpoch [91/256], Training Loss: 0.3944\nEpoch [91/256], Validation Loss: 0.4413, Validation Accuracy: 0.8050\nEpoch [92/256], Training Loss: 0.3922\nEpoch [92/256], Validation Loss: 0.4339, Validation Accuracy: 0.8083\nEpoch [93/256], Training Loss: 0.3912\nEpoch [93/256], Validation Loss: 0.4335, Validation Accuracy: 0.8027\nEpoch [94/256], Training Loss: 0.3895\nEpoch [94/256], Validation Loss: 0.4315, Validation Accuracy: 0.8048\nEpoch [95/256], Training Loss: 0.3886\nEpoch [95/256], Validation Loss: 0.4311, Validation Accuracy: 0.8054\nEpoch [96/256], Training Loss: 0.3878\nEpoch [96/256], Validation Loss: 0.4299, Validation Accuracy: 0.8069\nEpoch [97/256], Training Loss: 0.3858\nEpoch [97/256], Validation Loss: 0.4280, Validation Accuracy: 0.8069\nEpoch [98/256], Training Loss: 0.3858\nEpoch [98/256], Validation Loss: 0.4286, Validation Accuracy: 0.8075\nEpoch [99/256], Training Loss: 0.3840\nEpoch [99/256], Validation Loss: 0.4278, Validation Accuracy: 0.8074\nEpoch [100/256], Training Loss: 0.3825\nEpoch [100/256], Validation Loss: 0.4273, Validation Accuracy: 0.8075\nEpoch [101/256], Training Loss: 0.3815\nEpoch [101/256], Validation Loss: 0.4266, Validation Accuracy: 0.8090\nEpoch [102/256], Training Loss: 0.3802\nEpoch [102/256], Validation Loss: 0.4297, Validation Accuracy: 0.8083\nEpoch [103/256], Training Loss: 0.3798\nEpoch [103/256], Validation Loss: 0.4236, Validation Accuracy: 0.8090\nEpoch [104/256], Training Loss: 0.3786\nEpoch [104/256], Validation Loss: 0.4244, Validation Accuracy: 0.8115\nEpoch [105/256], Training Loss: 0.3770\nEpoch [105/256], Validation Loss: 0.4293, Validation Accuracy: 0.8123\nEpoch [106/256], Training Loss: 0.3764\nEpoch [106/256], Validation Loss: 0.4205, Validation Accuracy: 0.8120\nEpoch [107/256], Training Loss: 0.3757\nEpoch [107/256], Validation Loss: 0.4210, Validation Accuracy: 0.8106\nEpoch [108/256], Training Loss: 0.3743\nEpoch [108/256], Validation Loss: 0.4295, Validation Accuracy: 0.8048\nEpoch [109/256], Training Loss: 0.3729\nEpoch [109/256], Validation Loss: 0.4212, Validation Accuracy: 0.8120\nEpoch [110/256], Training Loss: 0.3716\nEpoch [110/256], Validation Loss: 0.4246, Validation Accuracy: 0.8110\nEpoch [111/256], Training Loss: 0.3703\nEpoch [111/256], Validation Loss: 0.4191, Validation Accuracy: 0.8133\nEpoch [112/256], Training Loss: 0.3703\nEpoch [112/256], Validation Loss: 0.4180, Validation Accuracy: 0.8162\nEpoch [113/256], Training Loss: 0.3686\nEpoch [113/256], Validation Loss: 0.4189, Validation Accuracy: 0.8168\nEpoch [114/256], Training Loss: 0.3671\nEpoch [114/256], Validation Loss: 0.4161, Validation Accuracy: 0.8139\nEpoch [115/256], Training Loss: 0.3649\nEpoch [115/256], Validation Loss: 0.4179, Validation Accuracy: 0.8150\nEpoch [116/256], Training Loss: 0.3645\nEpoch [116/256], Validation Loss: 0.4161, Validation Accuracy: 0.8141\nEpoch [117/256], Training Loss: 0.3640\nEpoch [117/256], Validation Loss: 0.4142, Validation Accuracy: 0.8162\nEpoch [118/256], Training Loss: 0.3619\nEpoch [118/256], Validation Loss: 0.4171, Validation Accuracy: 0.8170\nEpoch [119/256], Training Loss: 0.3608\nEpoch [119/256], Validation Loss: 0.4129, Validation Accuracy: 0.8160\nEpoch [120/256], Training Loss: 0.3598\nEpoch [120/256], Validation Loss: 0.4140, Validation Accuracy: 0.8152\nEpoch [121/256], Training Loss: 0.3604\nEpoch [121/256], Validation Loss: 0.4123, Validation Accuracy: 0.8174\nEpoch [122/256], Training Loss: 0.3581\nEpoch [122/256], Validation Loss: 0.4123, Validation Accuracy: 0.8150\nEpoch [123/256], Training Loss: 0.3564\nEpoch [123/256], Validation Loss: 0.4108, Validation Accuracy: 0.8189\nEpoch [124/256], Training Loss: 0.3562\nEpoch [124/256], Validation Loss: 0.4136, Validation Accuracy: 0.8206\nEpoch [125/256], Training Loss: 0.3554\nEpoch [125/256], Validation Loss: 0.4138, Validation Accuracy: 0.8152\nEpoch [126/256], Training Loss: 0.3533\nEpoch [126/256], Validation Loss: 0.4101, Validation Accuracy: 0.8163\nEpoch [127/256], Training Loss: 0.3537\nEpoch [127/256], Validation Loss: 0.4099, Validation Accuracy: 0.8194\nEpoch [128/256], Training Loss: 0.3518\nEpoch [128/256], Validation Loss: 0.4173, Validation Accuracy: 0.8110\nEpoch [129/256], Training Loss: 0.3519\nEpoch [129/256], Validation Loss: 0.4131, Validation Accuracy: 0.8198\nEpoch [130/256], Training Loss: 0.3499\nEpoch [130/256], Validation Loss: 0.4124, Validation Accuracy: 0.8142\nEpoch [131/256], Training Loss: 0.3482\nEpoch [131/256], Validation Loss: 0.4054, Validation Accuracy: 0.8216\nEpoch [132/256], Training Loss: 0.3483\nEpoch [132/256], Validation Loss: 0.4057, Validation Accuracy: 0.8216\nEpoch [133/256], Training Loss: 0.3464\nEpoch [133/256], Validation Loss: 0.4061, Validation Accuracy: 0.8173\nEpoch [134/256], Training Loss: 0.3455\nEpoch [134/256], Validation Loss: 0.4053, Validation Accuracy: 0.8232\nEpoch [135/256], Training Loss: 0.3454\nEpoch [135/256], Validation Loss: 0.4046, Validation Accuracy: 0.8232\nEpoch [136/256], Training Loss: 0.3432\nEpoch [136/256], Validation Loss: 0.4057, Validation Accuracy: 0.8158\nEpoch [137/256], Training Loss: 0.3427\nEpoch [137/256], Validation Loss: 0.4030, Validation Accuracy: 0.8227\nEpoch [138/256], Training Loss: 0.3420\nEpoch [138/256], Validation Loss: 0.4089, Validation Accuracy: 0.8216\nEpoch [139/256], Training Loss: 0.3408\nEpoch [139/256], Validation Loss: 0.4031, Validation Accuracy: 0.8232\nEpoch [140/256], Training Loss: 0.3402\nEpoch [140/256], Validation Loss: 0.4032, Validation Accuracy: 0.8226\nEpoch [141/256], Training Loss: 0.3402\nEpoch [141/256], Validation Loss: 0.4034, Validation Accuracy: 0.8165\nEpoch [142/256], Training Loss: 0.3396\nEpoch [142/256], Validation Loss: 0.4014, Validation Accuracy: 0.8210\nEpoch [143/256], Training Loss: 0.3373\nEpoch [143/256], Validation Loss: 0.4032, Validation Accuracy: 0.8226\nEpoch [144/256], Training Loss: 0.3362\nEpoch [144/256], Validation Loss: 0.4037, Validation Accuracy: 0.8246\nEpoch [145/256], Training Loss: 0.3350\nEpoch [145/256], Validation Loss: 0.4015, Validation Accuracy: 0.8240\nEpoch [146/256], Training Loss: 0.3340\nEpoch [146/256], Validation Loss: 0.4015, Validation Accuracy: 0.8211\nEpoch [147/256], Training Loss: 0.3350\nEpoch [147/256], Validation Loss: 0.4055, Validation Accuracy: 0.8245\nEpoch [148/256], Training Loss: 0.3321\nEpoch [148/256], Validation Loss: 0.4001, Validation Accuracy: 0.8218\nEpoch [149/256], Training Loss: 0.3319\nEpoch [149/256], Validation Loss: 0.3993, Validation Accuracy: 0.8248\nEpoch [150/256], Training Loss: 0.3317\nEpoch [150/256], Validation Loss: 0.4025, Validation Accuracy: 0.8246\nEpoch [151/256], Training Loss: 0.3307\nEpoch [151/256], Validation Loss: 0.3962, Validation Accuracy: 0.8242\nEpoch [152/256], Training Loss: 0.3293\nEpoch [152/256], Validation Loss: 0.3972, Validation Accuracy: 0.8253\nEpoch [153/256], Training Loss: 0.3278\nEpoch [153/256], Validation Loss: 0.3953, Validation Accuracy: 0.8238\nEpoch [154/256], Training Loss: 0.3284\nEpoch [154/256], Validation Loss: 0.3986, Validation Accuracy: 0.8227\nEpoch [155/256], Training Loss: 0.3257\nEpoch [155/256], Validation Loss: 0.3978, Validation Accuracy: 0.8246\nEpoch [156/256], Training Loss: 0.3257\nEpoch [156/256], Validation Loss: 0.3989, Validation Accuracy: 0.8262\nEpoch [157/256], Training Loss: 0.3250\nEpoch [157/256], Validation Loss: 0.3992, Validation Accuracy: 0.8262\nEpoch [158/256], Training Loss: 0.3238\nEpoch [158/256], Validation Loss: 0.3960, Validation Accuracy: 0.8270\nEpoch [159/256], Training Loss: 0.3239\nEpoch [159/256], Validation Loss: 0.3994, Validation Accuracy: 0.8237\nEpoch [160/256], Training Loss: 0.3224\nEpoch [160/256], Validation Loss: 0.4000, Validation Accuracy: 0.8192\nEpoch [161/256], Training Loss: 0.3220\nEpoch [161/256], Validation Loss: 0.3939, Validation Accuracy: 0.8232\nEpoch [162/256], Training Loss: 0.3210\nEpoch [162/256], Validation Loss: 0.3965, Validation Accuracy: 0.8254\nEpoch [163/256], Training Loss: 0.3202\nEpoch [163/256], Validation Loss: 0.3980, Validation Accuracy: 0.8274\nEpoch [164/256], Training Loss: 0.3195\nEpoch [164/256], Validation Loss: 0.3973, Validation Accuracy: 0.8296\nEpoch [165/256], Training Loss: 0.3191\nEpoch [165/256], Validation Loss: 0.3949, Validation Accuracy: 0.8254\nEpoch [166/256], Training Loss: 0.3180\nEpoch [166/256], Validation Loss: 0.3968, Validation Accuracy: 0.8282\nEpoch [167/256], Training Loss: 0.3168\nEpoch [167/256], Validation Loss: 0.3973, Validation Accuracy: 0.8274\nEpoch [168/256], Training Loss: 0.3168\nEpoch [168/256], Validation Loss: 0.3964, Validation Accuracy: 0.8278\nEpoch [169/256], Training Loss: 0.3151\nEpoch [169/256], Validation Loss: 0.3941, Validation Accuracy: 0.8270\nEpoch [170/256], Training Loss: 0.3145\nEpoch [170/256], Validation Loss: 0.3945, Validation Accuracy: 0.8266\nEpoch [171/256], Training Loss: 0.3141\nEpoch [171/256], Validation Loss: 0.3939, Validation Accuracy: 0.8275\nEpoch [172/256], Training Loss: 0.3131\nEpoch [172/256], Validation Loss: 0.3963, Validation Accuracy: 0.8307\nEpoch [173/256], Training Loss: 0.3116\nEpoch [173/256], Validation Loss: 0.3916, Validation Accuracy: 0.8264\nEpoch [174/256], Training Loss: 0.3111\nEpoch [174/256], Validation Loss: 0.3959, Validation Accuracy: 0.8293\nEpoch [175/256], Training Loss: 0.3119\nEpoch [175/256], Validation Loss: 0.3927, Validation Accuracy: 0.8286\nEpoch [176/256], Training Loss: 0.3103\nEpoch [176/256], Validation Loss: 0.3905, Validation Accuracy: 0.8269\nEpoch [177/256], Training Loss: 0.3098\nEpoch [177/256], Validation Loss: 0.3897, Validation Accuracy: 0.8280\nEpoch [178/256], Training Loss: 0.3097\nEpoch [178/256], Validation Loss: 0.3912, Validation Accuracy: 0.8280\nEpoch [179/256], Training Loss: 0.3082\nEpoch [179/256], Validation Loss: 0.3916, Validation Accuracy: 0.8288\nEpoch [180/256], Training Loss: 0.3069\nEpoch [180/256], Validation Loss: 0.3941, Validation Accuracy: 0.8290\nEpoch [181/256], Training Loss: 0.3077\nEpoch [181/256], Validation Loss: 0.3991, Validation Accuracy: 0.8296\nEpoch [182/256], Training Loss: 0.3051\nEpoch [182/256], Validation Loss: 0.3917, Validation Accuracy: 0.8256\nEpoch [183/256], Training Loss: 0.3067\nEpoch [183/256], Validation Loss: 0.3924, Validation Accuracy: 0.8298\nEpoch [184/256], Training Loss: 0.3047\nEpoch [184/256], Validation Loss: 0.3883, Validation Accuracy: 0.8286\nEpoch [185/256], Training Loss: 0.3044\nEpoch [185/256], Validation Loss: 0.3885, Validation Accuracy: 0.8291\nEpoch [186/256], Training Loss: 0.3029\nEpoch [186/256], Validation Loss: 0.3916, Validation Accuracy: 0.8298\nEpoch [187/256], Training Loss: 0.3023\nEpoch [187/256], Validation Loss: 0.3938, Validation Accuracy: 0.8317\nEpoch [188/256], Training Loss: 0.3027\nEpoch [188/256], Validation Loss: 0.3896, Validation Accuracy: 0.8250\nEpoch [189/256], Training Loss: 0.3020\nEpoch [189/256], Validation Loss: 0.3908, Validation Accuracy: 0.8312\nEpoch [190/256], Training Loss: 0.3004\nEpoch [190/256], Validation Loss: 0.3949, Validation Accuracy: 0.8312\nEpoch [191/256], Training Loss: 0.2997\nEpoch [191/256], Validation Loss: 0.3908, Validation Accuracy: 0.8325\nEpoch [192/256], Training Loss: 0.3008\nEpoch [192/256], Validation Loss: 0.3950, Validation Accuracy: 0.8242\nEpoch [193/256], Training Loss: 0.2986\nEpoch [193/256], Validation Loss: 0.3868, Validation Accuracy: 0.8294\nEpoch [194/256], Training Loss: 0.2981\nEpoch [194/256], Validation Loss: 0.3893, Validation Accuracy: 0.8298\nEpoch [195/256], Training Loss: 0.2974\nEpoch [195/256], Validation Loss: 0.3889, Validation Accuracy: 0.8259\nEpoch [196/256], Training Loss: 0.2961\nEpoch [196/256], Validation Loss: 0.3894, Validation Accuracy: 0.8307\nEpoch [197/256], Training Loss: 0.2958\nEpoch [197/256], Validation Loss: 0.3946, Validation Accuracy: 0.8320\nEpoch [198/256], Training Loss: 0.2949\nEpoch [198/256], Validation Loss: 0.3898, Validation Accuracy: 0.8291\nEpoch [199/256], Training Loss: 0.2946\nEpoch [199/256], Validation Loss: 0.3919, Validation Accuracy: 0.8331\nEpoch [200/256], Training Loss: 0.2933\nEpoch [200/256], Validation Loss: 0.3886, Validation Accuracy: 0.8302\nEpoch [201/256], Training Loss: 0.2951\nEpoch [201/256], Validation Loss: 0.3895, Validation Accuracy: 0.8323\nEpoch [202/256], Training Loss: 0.2922\nEpoch [202/256], Validation Loss: 0.3881, Validation Accuracy: 0.8304\nEpoch [203/256], Training Loss: 0.2918\nEpoch [203/256], Validation Loss: 0.3929, Validation Accuracy: 0.8346\nEpoch [204/256], Training Loss: 0.2931\nEpoch [204/256], Validation Loss: 0.3928, Validation Accuracy: 0.8285\nEpoch [205/256], Training Loss: 0.2915\nEpoch [205/256], Validation Loss: 0.3916, Validation Accuracy: 0.8333\nEpoch [206/256], Training Loss: 0.2899\nEpoch [206/256], Validation Loss: 0.3940, Validation Accuracy: 0.8294\nEpoch [207/256], Training Loss: 0.2894\nEpoch [207/256], Validation Loss: 0.3909, Validation Accuracy: 0.8322\nEpoch [208/256], Training Loss: 0.2895\nEpoch [208/256], Validation Loss: 0.3929, Validation Accuracy: 0.8323\nEpoch [209/256], Training Loss: 0.2905\nEpoch [209/256], Validation Loss: 0.3922, Validation Accuracy: 0.8344\nEpoch [210/256], Training Loss: 0.2871\nEpoch [210/256], Validation Loss: 0.3905, Validation Accuracy: 0.8349\nEpoch [211/256], Training Loss: 0.2869\nEpoch [211/256], Validation Loss: 0.3883, Validation Accuracy: 0.8333\nEpoch [212/256], Training Loss: 0.2867\nEpoch [212/256], Validation Loss: 0.3907, Validation Accuracy: 0.8339\nEpoch [213/256], Training Loss: 0.2859\nEpoch [213/256], Validation Loss: 0.3867, Validation Accuracy: 0.8330\nEpoch [214/256], Training Loss: 0.2862\nEpoch [214/256], Validation Loss: 0.3908, Validation Accuracy: 0.8342\nEpoch [215/256], Training Loss: 0.2840\nEpoch [215/256], Validation Loss: 0.3916, Validation Accuracy: 0.8341\nEpoch [216/256], Training Loss: 0.2846\nEpoch [216/256], Validation Loss: 0.3933, Validation Accuracy: 0.8365\nEpoch [217/256], Training Loss: 0.2842\nEpoch [217/256], Validation Loss: 0.4006, Validation Accuracy: 0.8338\nEpoch [218/256], Training Loss: 0.2830\nEpoch [218/256], Validation Loss: 0.3964, Validation Accuracy: 0.8269\nEpoch [219/256], Training Loss: 0.2849\nEpoch [219/256], Validation Loss: 0.3928, Validation Accuracy: 0.8354\nEpoch [220/256], Training Loss: 0.2827\nEpoch [220/256], Validation Loss: 0.3885, Validation Accuracy: 0.8347\nEpoch [221/256], Training Loss: 0.2818\nEpoch [221/256], Validation Loss: 0.3883, Validation Accuracy: 0.8349\nEpoch [222/256], Training Loss: 0.2806\nEpoch [222/256], Validation Loss: 0.3895, Validation Accuracy: 0.8347\nEpoch [223/256], Training Loss: 0.2807\nEpoch [223/256], Validation Loss: 0.3861, Validation Accuracy: 0.8360\nEpoch [224/256], Training Loss: 0.2800\nEpoch [224/256], Validation Loss: 0.3882, Validation Accuracy: 0.8339\nEpoch [225/256], Training Loss: 0.2791\nEpoch [225/256], Validation Loss: 0.3952, Validation Accuracy: 0.8288\nEpoch [226/256], Training Loss: 0.2782\nEpoch [226/256], Validation Loss: 0.3914, Validation Accuracy: 0.8352\nEpoch [227/256], Training Loss: 0.2782\nEpoch [227/256], Validation Loss: 0.3905, Validation Accuracy: 0.8342\nEpoch [228/256], Training Loss: 0.2792\nEpoch [228/256], Validation Loss: 0.3961, Validation Accuracy: 0.8336\nEpoch [229/256], Training Loss: 0.2778\nEpoch [229/256], Validation Loss: 0.3987, Validation Accuracy: 0.8360\nEpoch [230/256], Training Loss: 0.2776\nEpoch [230/256], Validation Loss: 0.4023, Validation Accuracy: 0.8336\nEpoch [231/256], Training Loss: 0.2755\nEpoch [231/256], Validation Loss: 0.3955, Validation Accuracy: 0.8347\nEpoch [232/256], Training Loss: 0.2764\nEpoch [232/256], Validation Loss: 0.3947, Validation Accuracy: 0.8371\nEpoch [233/256], Training Loss: 0.2754\nEpoch [233/256], Validation Loss: 0.3993, Validation Accuracy: 0.8232\nEpoch [234/256], Training Loss: 0.2745\nEpoch [234/256], Validation Loss: 0.3924, Validation Accuracy: 0.8366\nEpoch [235/256], Training Loss: 0.2731\nEpoch [235/256], Validation Loss: 0.3977, Validation Accuracy: 0.8360\nEpoch [236/256], Training Loss: 0.2744\nEpoch [236/256], Validation Loss: 0.3954, Validation Accuracy: 0.8342\nEpoch [237/256], Training Loss: 0.2737\nEpoch [237/256], Validation Loss: 0.3871, Validation Accuracy: 0.8338\nEpoch [238/256], Training Loss: 0.2725\nEpoch [238/256], Validation Loss: 0.3988, Validation Accuracy: 0.8358\nEpoch [239/256], Training Loss: 0.2724\nEpoch [239/256], Validation Loss: 0.3923, Validation Accuracy: 0.8378\nEpoch [240/256], Training Loss: 0.2716\nEpoch [240/256], Validation Loss: 0.3934, Validation Accuracy: 0.8368\nEpoch [241/256], Training Loss: 0.2708\nEpoch [241/256], Validation Loss: 0.4109, Validation Accuracy: 0.8320\nEpoch [242/256], Training Loss: 0.2701\nEpoch [242/256], Validation Loss: 0.3937, Validation Accuracy: 0.8360\nEpoch [243/256], Training Loss: 0.2705\nEpoch [243/256], Validation Loss: 0.3908, Validation Accuracy: 0.8370\nEpoch [244/256], Training Loss: 0.2692\nEpoch [244/256], Validation Loss: 0.3949, Validation Accuracy: 0.8357\nEpoch [245/256], Training Loss: 0.2682\nEpoch [245/256], Validation Loss: 0.3901, Validation Accuracy: 0.8354\nEpoch [246/256], Training Loss: 0.2673\nEpoch [246/256], Validation Loss: 0.3904, Validation Accuracy: 0.8366\nEpoch [247/256], Training Loss: 0.2673\nEpoch [247/256], Validation Loss: 0.3900, Validation Accuracy: 0.8376\nEpoch [248/256], Training Loss: 0.2665\nEpoch [248/256], Validation Loss: 0.3909, Validation Accuracy: 0.8366\nEpoch [249/256], Training Loss: 0.2662\nEpoch [249/256], Validation Loss: 0.3948, Validation Accuracy: 0.8357\nEpoch [250/256], Training Loss: 0.2648\nEpoch [250/256], Validation Loss: 0.4024, Validation Accuracy: 0.8338\nEpoch [251/256], Training Loss: 0.2665\nEpoch [251/256], Validation Loss: 0.4019, Validation Accuracy: 0.8277\nEpoch [252/256], Training Loss: 0.2652\nEpoch [252/256], Validation Loss: 0.4017, Validation Accuracy: 0.8373\nEpoch [253/256], Training Loss: 0.2641\nEpoch [253/256], Validation Loss: 0.4036, Validation Accuracy: 0.8371\nEpoch [254/256], Training Loss: 0.2643\nEpoch [254/256], Validation Loss: 0.3902, Validation Accuracy: 0.8382\nEpoch [255/256], Training Loss: 0.2638\nEpoch [255/256], Validation Loss: 0.3973, Validation Accuracy: 0.8328\nEpoch [256/256], Training Loss: 0.2638\nEpoch [256/256], Validation Loss: 0.3942, Validation Accuracy: 0.8363\nTraining and Validation completed!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Evaluate the model on the test set.","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Variables for total loss and number of correct predictions\ntotal_loss = 0.0\ncorrect_predictions = 0\n\n# Iterate through the test data\nwith torch.no_grad():  # Disable gradient computation during evaluation\n    for inputs, labels in test_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Reshape the model's output\n        outputs = outputs.view(-1)  # Change dimensions from [batch_size, 1] to [batch_size]\n        \n        # Compute the loss\n        loss = criterion(outputs, labels.float())  # Convert labels to float\n        \n        # Update the total loss\n        total_loss += loss.item()\n        \n        # Calculate the number of correct predictions\n        threshold = 0.5\n        predicted_labels = (torch.sigmoid(outputs) > threshold).float()\n        correct_predictions += (predicted_labels == labels.float()).sum().item()\n        \n# Calculate the average loss for the test set\naverage_loss = total_loss / len(test_loader)\n\n# Calculate accuracy on the test set\naccuracy = correct_predictions / len(test_loader.dataset)\n\nprint(f'Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}')\n\n# Log metrics using WandB\nwandb.log({\"Test Loss\": average_loss, \"Test Accuracy\": accuracy})\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-01-29T18:33:45.667865Z","iopub.execute_input":"2024-01-29T18:33:45.669471Z","iopub.status.idle":"2024-01-29T18:33:46.313386Z","shell.execute_reply.started":"2024-01-29T18:33:45.669431Z","shell.execute_reply":"2024-01-29T18:33:46.312098Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"Test Loss: 0.3586, Test Accuracy: 0.8507\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-01-29T18:33:46.314948Z","iopub.execute_input":"2024-01-29T18:33:46.315392Z","iopub.status.idle":"2024-01-29T18:33:51.694596Z","shell.execute_reply.started":"2024-01-29T18:33:46.315345Z","shell.execute_reply":"2024-01-29T18:33:51.693561Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stderr","text":"wandb: WARNING No program path found when generating artifact job source for a non-colab notebook run. See https://docs.wandb.ai/guides/launch/create-job\nwandb: WARNING Source type is set to 'artifact' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.250 MB of 0.469 MB uploaded\\r'), FloatProgress(value=0.5333447210715861, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>LR</td><td>▁</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Training Loss</td><td>██▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▂▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>Validation Loss</td><td>██▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>256</td></tr><tr><td>LR</td><td>1e-05</td></tr><tr><td>Test Accuracy</td><td>0.85072</td></tr><tr><td>Test Loss</td><td>0.35856</td></tr><tr><td>Training Loss</td><td>0.26377</td></tr><tr><td>Validation Accuracy</td><td>0.83632</td></tr><tr><td>Validation Loss</td><td>0.39418</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bright-water-26</strong> at: <a href='https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb' target=\"_blank\">https://wandb.ai/ales-2000-09/IMDB%20Dataset%20of%2050K%20Movie%20Reviews/runs/p93ykrxb</a><br/>Synced 6 W&B file(s), 0 media file(s), 10 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240129_182106-p93ykrxb/logs</code>"},"metadata":{}}]}]}